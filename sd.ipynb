{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 4085\n",
      "Number of testing images: 638\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ShadowDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "\n",
    "        mask_name = image_name.replace(\".jpg\", \".png\")\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"Warning: Mask not found for {image_name}. Expected: {mask_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.image_filenames))\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "train_images_dir = \"sbu/SBUTrain4KRecoveredSmall/ShadowImages\"\n",
    "train_masks_dir = \"sbu/SBUTrain4KRecoveredSmall/ShadowMasks\"\n",
    "test_images_dir = \"sbu/SBU-Test/ShadowImages\"\n",
    "test_masks_dir = \"sbu/SBU-Test/ShadowMasks\"\n",
    "\n",
    "train_dataset = ShadowDataset(train_images_dir, train_masks_dir, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = ShadowDataset(test_images_dir, test_masks_dir, transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Number of training images: {len(train_dataset)}\")\n",
    "print(f\"Number of testing images: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 511 batches.\n",
      "Epoch [1/25], Loss: 0.2875\n",
      "Epoch [2/25], Loss: 0.2233\n",
      "Epoch [3/25], Loss: 0.2118\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.decoder1 = self.conv_block(256, 128)\n",
    "        self.decoder2 = self.conv_block(128, 64)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, kernel_size=2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, kernel_size=2))\n",
    "        dec1 = self.decoder1(F.interpolate(enc3, enc2.size()[2:]))\n",
    "        dec2 = self.decoder2(F.interpolate(dec1, enc1.size()[2:]))\n",
    "        return self.final_conv(dec2)\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "model = UNet().to(device)\n",
    "\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "print(f\"Starting training with {len(train_loader)} batches.\", flush=True)\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(f\"Starting evaluation with {len(test_loader)} batches.\", flush=True)\n",
    "    for images, masks in test_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = model(images)\n",
    "        predicted_masks = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[0].permute(1, 2, 0).cpu())\n",
    "        plt.title(\"Input Image\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(masks[0].cpu(), cmap=\"gray\")\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(predicted_masks[0].cpu(), cmap=\"gray\")\n",
    "        plt.title(\"Predicted Mask\")\n",
    "\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model (make sure to adjust the path if needed)\n",
    "# model = UNet().to(device)  # Use the same model definition as earlier\n",
    "# model.load_state_dict(torch.load(\"path/to/your/model.pth\"))  # Load your trained model state\n",
    "# model.eval()\n",
    "\n",
    "# Load and preprocess the test image\n",
    "image_path = \"crosswalk.jpg\"  # Update the path to your test image\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Define the same transformations as during training\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),  # Make sure this matches your training transform\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the image\n",
    "input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "input_image = input_image.to(device)  # Move to the same device as the model\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    output = model(input_image)\n",
    "    predicted_mask = torch.sigmoid(output) > 0.5  # Binarize output\n",
    "\n",
    "# Convert predicted mask to a more visualizable format\n",
    "predicted_mask = (\n",
    "    predicted_mask.squeeze(0).cpu().numpy()\n",
    ")  # Remove batch dimension and convert to ndarray\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_mask, cmap=\"gray\")\n",
    "plt.title(\"Predicted Shadow Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
